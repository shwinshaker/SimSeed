# ---- preset paras ---- (for config only)
test: True  # test mode for quick sanity check
# test setting:
#    probe traing epoch = 1, split = 2, encoding = 64, batch size = 32x1
#    dataset summary show example = False
#    get_pseudo_label verbose = False
#    huggingface warning = none
suffix:  # suffix to name the workdir

trial:

# -- data --
# train_subset_path: '/home/chengyu/bert_classification/data/nyt-fine/id_pseudo_weak.npy'  # indices to specify the training subset

# -- regularizer --
# label_smoothing: 0.5
# loss_flooding: 0.1 # 
gradient_clipping: True

adremove: False  # True
adremove_alpha: 0.5  # 1.0: no regular loss

randremove: False  # True
randremove_alpha: 0.5  # 1.0: no regular loss
randremove_num: 0.1  # ratio of removed tokens in a sequence

mlmreplace: False  # True
mlmreplace_alpha: 0.5  # 1.0: no regular loss
randremove_num: 0.1  # ratio of masked tokens in a sequence

augparaphrase: False
augparaphrase_alpha: 0.5
augparaphrase_temperature: 2

# -- trackers
backdoorTrack: True
seedwordAdTrack: True
exTrack_eval_subset: False  # including the rest of training set after random subset sampling
exTrackOptions_eval_subset: ['record_softmax_last']

# -- pseudo label from weak supervision -- 
pseudo_weak_sup: True
pseudo_weak_sup_select: True  # only works if bootstrapping is turned on, otherwise no model is available to select pseudo_labels
pseudo_weak_sup_select_metric: confidence
pseudo_weak_sup_select_train_epochs: 4
pseudo_weak_sup_select_train_aug: randremove
pseudo_weak_sup_select_eval_aug: none
pseudo_weak_sup_select_aug_randremove_ratio: 0.99
pseudo_weak_sup_select_aug_paraphrase_temperature: 4
pseudo_weak_sup_select_aug_mlmreplace_ratio: 0.9
pseudo_weak_sup_select_threshold: 0.5
pseudo_weak_sup_select_threshold_type: 'percentile'
pseudo_weak_sup_select_class_balance: True

# -- bootstrap --
bootstrap: True
bootstrap_n_iteration: 5
bootstrap_threshold_func: linear_max=1.0  # linear_max=0.83 # linear_max=0.69 #  constant
bootstrap_epoch_func: constant

# -- pseudo label --
pseudo_model_path:  # if bootstrap, will be replaced by bootstrap iter dirs
pseudo_unlabeled_idx_path:  # if none, use all unlabeled training set
pseudo_threshold: 0.  # 0.6 # 0.093 # 0.077 # 
pseudo_threshold_type: 'percentile'  # 'value' 
pseudo_class_balance: True  # class balanced selection

# -- pseudo-labeling
save_pseudo_label: True
save_pseudo_label_unlabeled_idx_path:  # if none, use all unlabeled training set
save_pseudo_label_epoch: False  # True

# ----- Regular paras ------
checkpoint_dir: 'checkpoints'
dataset: 20news-fine
encoding_max_length: 512
test_ratio: 0.15  # select a random subset for testing
class_balanced_sampling: True
data_dir: '/home/chengyu/bert_classification/data'
opt: adamw
model: bert-base-uncased
scheduler: linear
resume: False
epochs: 4
lr: 2e-5
batch_size: 4
update_freq: 8  # grad accumulation to reduce peak memory
wd: 0
momentum:
gamma:

gpu_id: 1
manual_seed:
state_path:
save_model: False  # Save the model after training?
save_checkpoint: False  # save checkpoint after training?


